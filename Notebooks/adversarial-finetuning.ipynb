{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import wandb\ntoken=\"dbc1309d957850844048db2d2add36cbefe62171\"\nwandb.login(key=token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:29:49.220154Z","iopub.execute_input":"2024-12-20T18:29:49.220549Z","iopub.status.idle":"2024-12-20T18:29:51.468511Z","shell.execute_reply.started":"2024-12-20T18:29:49.220501Z","shell.execute_reply":"2024-12-20T18:29:51.467603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:29:51.470432Z","iopub.execute_input":"2024-12-20T18:29:51.471420Z","iopub.status.idle":"2024-12-20T18:30:05.091688Z","shell.execute_reply.started":"2024-12-20T18:29:51.471374Z","shell.execute_reply":"2024-12-20T18:30:05.090269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import get_dataset_config_names\nfrom datasets import load_dataset\nfrom collections import defaultdict, Counter\nfrom datasets import DatasetDict\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom transformers import XLMRobertaConfig, AutoConfig, AutoTokenizer, DataCollatorForTokenClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\nfrom seqeval.metrics import classification_report, f1_score, precision_score, recall_score\nfrom torch.nn.functional import cross_entropy\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nfrom transformers import TrainerCallback\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\nfrom tqdm import tqdm\nfrom datasets import concatenate_datasets\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:23:39.594235Z","iopub.execute_input":"2024-12-20T20:23:39.594896Z","iopub.status.idle":"2024-12-20T20:23:39.602352Z","shell.execute_reply.started":"2024-12-20T20:23:39.594856Z","shell.execute_reply":"2024-12-20T20:23:39.601232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preparation and Model Building","metadata":{}},{"cell_type":"code","source":"def get_data():\n    \n    langs = [\"de\", \"fr\", \"it\", \"en\"]\n    panx_ch = defaultdict(DatasetDict)\n    \n    for lang in langs:\n        ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n        for split in ds: \n            panx_ch[lang][split] = ds[split].shuffle(seed=0)\n        \n    return panx_ch\n\npanx_ch = get_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:24.594092Z","iopub.execute_input":"2024-12-20T18:30:24.594729Z","iopub.status.idle":"2024-12-20T18:30:38.367645Z","shell.execute_reply.started":"2024-12-20T18:30:24.594695Z","shell.execute_reply":"2024-12-20T18:30:38.366837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:38.368680Z","iopub.execute_input":"2024-12-20T18:30:38.368919Z","iopub.status.idle":"2024-12-20T18:30:38.373728Z","shell.execute_reply.started":"2024-12-20T18:30:38.368894Z","shell.execute_reply":"2024-12-20T18:30:38.372558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_tag_names(batch):\n return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\npanx_de = panx_ch[\"de\"].map(create_tag_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:38.374939Z","iopub.execute_input":"2024-12-20T18:30:38.375217Z","iopub.status.idle":"2024-12-20T18:30:42.859293Z","shell.execute_reply.started":"2024-12-20T18:30:38.375189Z","shell.execute_reply":"2024-12-20T18:30:42.858197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n    \"\"\"\n    A token classification model based on XLM-Roberta.\n\n    This model is designed for token-level tasks such as Named Entity Recognition (NER). \n    It uses XLM-Roberta as the backbone and adds a classification head on top for predicting \n    token labels.\n\n    Attributes:\n        config_class: The configuration class for XLM-Roberta.\n        num_labels (int): Number of labels for the classification task.\n        roberta (RobertaModel): The XLM-Roberta model without the pooling layer.\n        dropout (nn.Dropout): Dropout layer for regularization.\n        classifier (nn.Linear): A linear layer for mapping hidden states to label logits.\n\n    Args:\n        config (XLMRobertaConfig): Configuration object containing the model's parameters.\n\n    Methods:\n        forward(input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n            Forward pass of the model.\n\n            Args:\n                input_ids (torch.Tensor): Tensor of input token IDs of shape `(batch_size, sequence_length)`.\n                attention_mask (torch.Tensor, optional): Mask tensor of shape `(batch_size, sequence_length)` \n                                                         indicating which tokens to attend to.\n                token_type_ids (torch.Tensor, optional): Tensor of shape `(batch_size, sequence_length)` \n                                                        specifying token types (not typically used in Roberta models).\n                labels (torch.Tensor, optional): Tensor of shape `(batch_size, sequence_length)` containing \n                                                 the true labels for each token.\n\n            Returns:\n                TokenClassifierOutput: An output object containing:\n                    - `loss` (torch.Tensor, optional): The computed loss if `labels` are provided.\n                    - `logits` (torch.Tensor): Logits of shape `(batch_size, sequence_length, num_labels)`.\n                    - `hidden_states` (tuple, optional): Hidden states from the backbone model.\n                    - `attentions` (tuple, optional): Attention weights from the backbone model.\n    \"\"\"\n    \n    config_class = XLMRobertaConfig\n    \n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.init_weights()\n        \n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids, **kwargs)\n        sequence_output = self.dropout(outputs[0])\n        logits = self.classifier(sequence_output)\n        loss = None\n        if labels is not None:\n            loss_fun = nn.CrossEntropyLoss()\n            loss = loss_fun(logits.view(-1, self.num_labels), labels.view(-1))\n            \n        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:42.861007Z","iopub.execute_input":"2024-12-20T18:30:42.861625Z","iopub.status.idle":"2024-12-20T18:30:42.870862Z","shell.execute_reply.started":"2024-12-20T18:30:42.861570Z","shell.execute_reply":"2024-12-20T18:30:42.869760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag: idx for idx, tag in enumerate(tags.names)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:42.872398Z","iopub.execute_input":"2024-12-20T18:30:42.872838Z","iopub.status.idle":"2024-12-20T18:30:42.902775Z","shell.execute_reply.started":"2024-12-20T18:30:42.872794Z","shell.execute_reply":"2024-12-20T18:30:42.901521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xlmr_config = AutoConfig.from_pretrained(\"xlm-roberta-base\", num_labels=tags.num_classes, id2label=index2tag, label2id=tag2index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:42.904676Z","iopub.execute_input":"2024-12-20T18:30:42.905001Z","iopub.status.idle":"2024-12-20T18:30:43.018812Z","shell.execute_reply.started":"2024-12-20T18:30:42.904956Z","shell.execute_reply":"2024-12-20T18:30:43.017761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:43.021797Z","iopub.execute_input":"2024-12-20T18:30:43.022120Z","iopub.status.idle":"2024-12-20T18:30:43.083808Z","shell.execute_reply.started":"2024-12-20T18:30:43.022090Z","shell.execute_reply":"2024-12-20T18:30:43.082631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:43.085288Z","iopub.execute_input":"2024-12-20T18:30:43.085669Z","iopub.status.idle":"2024-12-20T18:30:45.496699Z","shell.execute_reply.started":"2024-12-20T18:30:43.085636Z","shell.execute_reply":"2024-12-20T18:30:45.495891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tag_text(text, tags, model, tokenizer):\n    \"\"\"\n    Tags a given text with predictions from a NER model.\n\n    This function tokenizes the input text, runs it through a NER model, and \n    generates predictions for each token. The predictions are converted into \n    human-readable tag names, and the tokens and their corresponding tags are \n    returned as a pandas DataFrame.\n\n    Args:\n        text (str): The input text to be tagged.\n        tags: A mapping or object containing tag names (e.g., `tags.names`).\n        model: A pre-trained NER model that takes tokenized inputs and \n               outputs logits for each token.\n        tokenizer: A tokenizer that splits the input text into tokens \n                   compatible with the model.\n    \n    Returns:\n        pd.DataFrame: A DataFrame containing:\n            - \"Tokens\": List of tokens from the input text.\n            - \"Tags\": Predicted tags corresponding to each token.\n    \"\"\"\n    tokens = tokenizer(text).tokens()\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    outputs = model(input_ids)[0]\n    predictions = torch.argmax(outputs, dim=2)\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:45.497992Z","iopub.execute_input":"2024-12-20T18:30:45.498358Z","iopub.status.idle":"2024-12-20T18:30:45.505435Z","shell.execute_reply.started":"2024-12-20T18:30:45.498318Z","shell.execute_reply":"2024-12-20T18:30:45.504297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    \"\"\"\n    Tokenizes input sentences and aligns NER labels with tokenized outputs.\n\n    This function uses a tokenizer that supports word-level tokenization and aligns \n    the NER tags to the subword tokenization scheme. It assigns `-100` to subword \n    tokens or special tokens to ensure they are ignored during the loss computation.\n\n    Args:\n        examples (dict): A dictionary containing:\n            - \"tokens\" (list of list of str): Sentences represented as lists of tokens.\n            - \"ner_tags\" (list of list of int): Corresponding NER tags for the tokens.\n\n    Returns:\n        dict: A dictionary containing:\n            - Tokenized inputs (e.g., \"input_ids\", \"attention_mask\").\n            - \"labels\": Aligned labels for the tokenized inputs.\n    \"\"\"\n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, \n                                      is_split_into_words=True)\n    labels = []\n    for idx, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:45.506682Z","iopub.execute_input":"2024-12-20T18:30:45.506956Z","iopub.status.idle":"2024-12-20T18:30:45.522811Z","shell.execute_reply.started":"2024-12-20T18:30:45.506929Z","shell.execute_reply":"2024-12-20T18:30:45.521933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_panx_dataset(corpus):\n    \"\"\"\n    Encodes a PAN-X dataset by tokenizing the input sentences and aligning NER labels.\n\n    This function applies the `tokenize_and_align_labels` function to the dataset using \n    batched processing, removing unnecessary columns (e.g., 'tokens', 'ner_tags', 'langs') \n    to prepare the dataset for model training.\n\n    Args:\n        corpus (DatasetDict): A `DatasetDict` object containing splits (e.g., 'train', \n                              'validation', 'test') with the features:\n                              - \"tokens\": List of tokens for each sentence.\n                              - \"ner_tags\": NER labels for the tokens.\n                              - \"langs\": Language identifiers.\n\n    Returns:\n        DatasetDict: A `DatasetDict` with tokenized inputs and aligned labels, \n                     containing features such as \"input_ids\", \"attention_mask\", \n                     and \"labels\".\n    \"\"\"\n    return corpus.map(tokenize_and_align_labels, batched=True, \n                      remove_columns=['tokens', 'ner_tags', 'langs'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:45.523946Z","iopub.execute_input":"2024-12-20T18:30:45.524218Z","iopub.status.idle":"2024-12-20T18:30:45.540410Z","shell.execute_reply.started":"2024-12-20T18:30:45.524192Z","shell.execute_reply":"2024-12-20T18:30:45.539574Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## zero-shot cross-lingual transfer from German","metadata":{}},{"cell_type":"code","source":"panx_en_encoded = encode_panx_dataset(panx_ch[\"en\"])\npanx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])\npanx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\npanx_it_encoded = encode_panx_dataset(panx_ch[\"it\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:30:45.541319Z","iopub.execute_input":"2024-12-20T18:30:45.541575Z","iopub.status.idle":"2024-12-20T18:31:00.339841Z","shell.execute_reply.started":"2024-12-20T18:30:45.541544Z","shell.execute_reply":"2024-12-20T18:31:00.339026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def align_predictions(predictions, label_ids):\n    \"\"\"\n    Aligns model predictions with their corresponding labels, excluding ignored indices.\n\n    This function processes batched model predictions and label IDs, converting them \n    into human-readable tag names while skipping indices marked with `-100` (ignored labels). \n    It ensures that predictions and labels are aligned at the token level.\n\n    Args:\n        predictions (numpy.ndarray): Array of shape `(batch_size, seq_len, num_labels)` \n                                     containing the model's logits for each token.\n        label_ids (numpy.ndarray): Array of shape `(batch_size, seq_len)` containing \n                                   the true label IDs for each token, with `-100` \n                                   indicating ignored tokens.\n\n    Returns:\n        tuple: A pair of lists:\n            - preds_list (list of list of str): Predicted tags for each example in the batch.\n            - labels_list (list of list of str): True tags for each example in the batch.\n    \"\"\"\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            # Ignore label IDs = -100\n            if label_ids[batch_idx, seq_idx] != -100:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n                \n        labels_list.append(example_labels)\n        preds_list.append(example_preds)\n        \n    return preds_list, labels_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.340920Z","iopub.execute_input":"2024-12-20T18:31:00.341199Z","iopub.status.idle":"2024-12-20T18:31:00.348361Z","shell.execute_reply.started":"2024-12-20T18:31:00.341172Z","shell.execute_reply":"2024-12-20T18:31:00.347564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n    return {\"f1\": f1_score(y_true, y_pred), \"precision\":precision_score(y_true, y_pred),\"recall\":recall_score(y_true, y_pred)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.349665Z","iopub.execute_input":"2024-12-20T18:31:00.350025Z","iopub.status.idle":"2024-12-20T18:31:00.394815Z","shell.execute_reply.started":"2024-12-20T18:31:00.349985Z","shell.execute_reply":"2024-12-20T18:31:00.393906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.396461Z","iopub.execute_input":"2024-12-20T18:31:00.396821Z","iopub.status.idle":"2024-12-20T18:31:00.406930Z","shell.execute_reply.started":"2024-12-20T18:31:00.396782Z","shell.execute_reply":"2024-12-20T18:31:00.406037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def model_init():\n    return (XLMRobertaForTokenClassification.from_pretrained(\"xlm-roberta-base\", config=xlmr_config).to(device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.408076Z","iopub.execute_input":"2024-12-20T18:31:00.408995Z","iopub.status.idle":"2024-12-20T18:31:00.419138Z","shell.execute_reply.started":"2024-12-20T18:31:00.408952Z","shell.execute_reply":"2024-12-20T18:31:00.418276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_f1_score(trainer, dataset):\n return trainer.predict(dataset).metrics[\"test_f1\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.420047Z","iopub.execute_input":"2024-12-20T18:31:00.420257Z","iopub.status.idle":"2024-12-20T18:31:00.431736Z","shell.execute_reply.started":"2024-12-20T18:31:00.420236Z","shell.execute_reply":"2024-12-20T18:31:00.430723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"panx_en_encoded = panx_en_encoded.with_format(\"torch\")\npanx_fr_encoded = panx_fr_encoded.with_format(\"torch\")\npanx_de_encoded = panx_de_encoded.with_format(\"torch\")\npanx_it_encoded = panx_it_encoded.with_format(\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.432837Z","iopub.execute_input":"2024-12-20T18:31:00.433282Z","iopub.status.idle":"2024-12-20T18:31:00.447958Z","shell.execute_reply.started":"2024-12-20T18:31:00.433242Z","shell.execute_reply":"2024-12-20T18:31:00.447062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"en_loaders = {\n    \"train\": DataLoader(panx_en_encoded[\"train\"], batch_size=16, collate_fn=data_collator),\n    \"validation\": DataLoader(panx_en_encoded[\"validation\"], batch_size=16, collate_fn=data_collator),\n    \"test\": DataLoader(panx_en_encoded[\"test\"], batch_size=16, collate_fn=data_collator)\n}\n\nfr_loaders = {\n    \"train\": DataLoader(panx_fr_encoded[\"train\"], batch_size=16, collate_fn=data_collator),\n    \"validation\": DataLoader(panx_fr_encoded[\"validation\"], batch_size=16, collate_fn=data_collator),\n    \"test\": DataLoader(panx_fr_encoded[\"test\"], batch_size=16, collate_fn=data_collator)\n}\n\nde_loaders = {\n    \"train\": DataLoader(panx_de_encoded[\"train\"], batch_size=16, collate_fn=data_collator),\n    \"validation\": DataLoader(panx_de_encoded[\"validation\"], batch_size=16, collate_fn=data_collator),\n    \"test\": DataLoader(panx_de_encoded[\"test\"], batch_size=16, collate_fn=data_collator)\n}\n\nit_loaders = {\n    \"train\": DataLoader(panx_it_encoded[\"train\"], batch_size=16, collate_fn=data_collator),\n    \"validation\": DataLoader(panx_it_encoded[\"validation\"], batch_size=16, collate_fn=data_collator),\n    \"test\": DataLoader(panx_it_encoded[\"test\"], batch_size=16, collate_fn=data_collator)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.449042Z","iopub.execute_input":"2024-12-20T18:31:00.449355Z","iopub.status.idle":"2024-12-20T18:31:00.459748Z","shell.execute_reply.started":"2024-12-20T18:31:00.449313Z","shell.execute_reply":"2024-12-20T18:31:00.458903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, data_loaders, optimizer, device):\n    model.train()\n    epoch_loss = {lang: 0 for lang in data_loaders}\n\n    # For each language, compute loss and track the worst loss\n    for lang, loaders in data_loaders.items():\n        for batch in tqdm(loaders[\"train\"], desc=f\"Training {lang}\", leave=False):\n            # Move batch to device\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            # Forward pass through the model\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            epoch_loss[lang] += loss.item()\n\n    # Average the loss for each language over the epoch\n    for lang in epoch_loss:\n        epoch_loss[lang] /= len(data_loaders[lang][\"train\"])\n\n    # Identify the language with the worst (highest) loss\n    worst_loss_lang = max(epoch_loss, key=epoch_loss.get)  # Get language with the worst loss\n    print(f\"Worst Loss Language: {worst_loss_lang} with loss {epoch_loss[worst_loss_lang]}\")\n\n    # Now perform the backward pass only for the worst loss language\n    optimizer.zero_grad()  # Zero the gradients before the backward pass\n\n    # Perform the forward pass for the batch from the worst loss language\n    for batch in tqdm(data_loaders[worst_loss_lang][\"train\"], desc=f\"Backward on {worst_loss_lang}\", leave=False):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Forward pass through the model\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        \n        # Backward pass (only for the worst loss batch)\n        loss.backward()\n\n    # Perform optimizer step\n    optimizer.step()\n\n    return epoch_loss, worst_loss_lang\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.460909Z","iopub.execute_input":"2024-12-20T18:31:00.461206Z","iopub.status.idle":"2024-12-20T18:31:00.471042Z","shell.execute_reply.started":"2024-12-20T18:31:00.461180Z","shell.execute_reply":"2024-12-20T18:31:00.470128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate(model, data_loaders, device):\n    model.eval()\n    val_loss = {lang: 0 for lang in data_loaders}\n\n    with torch.no_grad():\n        for lang, loaders in data_loaders.items():\n            for batch in tqdm(loaders[\"validation\"], desc=f\"Validating {lang}\", leave=False):\n                # Move batch to device\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n                labels = batch[\"labels\"].to(device)\n\n                # Forward pass through the model\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                val_loss[lang] += loss.item()\n\n    # Calculate average validation loss\n    for lang in val_loss:\n        val_loss[lang] /= len(data_loaders[lang][\"validation\"])\n\n    return val_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.472368Z","iopub.execute_input":"2024-12-20T18:31:00.473023Z","iopub.status.idle":"2024-12-20T18:31:00.488051Z","shell.execute_reply.started":"2024-12-20T18:31:00.472961Z","shell.execute_reply":"2024-12-20T18:31:00.487094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_loop(model, data_loaders, optimizer, device, epochs=5):\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n\n        # Train for one epoch\n        train_loss, worst_loss_lang = train_one_epoch(model, data_loaders, optimizer, device)\n        print(f\"Train Loss per Language: {train_loss}\")\n        print(f\"Worst loss at {worst_loss_lang}.\")\n\n        # Perform validation after each epoch\n        val_loss = validate(model, data_loaders, device)\n        print(f\"Validation Loss per Language: {val_loss}\")\n\n\n    return model  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.489168Z","iopub.execute_input":"2024-12-20T18:31:00.489492Z","iopub.status.idle":"2024-12-20T18:31:00.513539Z","shell.execute_reply.started":"2024-12-20T18:31:00.489455Z","shell.execute_reply":"2024-12-20T18:31:00.512389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_loaders = {\n    \"en\": en_loaders,\n    \"fr\": fr_loaders,\n    \"de\": de_loaders,\n    \"it\": it_loaders\n}\nmodel = model_init()\noptimizer = Adam(model.parameters(), lr=1e-5) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:00.515012Z","iopub.execute_input":"2024-12-20T18:31:00.515446Z","iopub.status.idle":"2024-12-20T18:31:06.273939Z","shell.execute_reply.started":"2024-12-20T18:31:00.515382Z","shell.execute_reply":"2024-12-20T18:31:06.272815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = train_loop(model, data_loaders, optimizer, device, epochs=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T18:31:06.275351Z","iopub.execute_input":"2024-12-20T18:31:06.275773Z","iopub.status.idle":"2024-12-20T19:56:18.487386Z","shell.execute_reply.started":"2024-12-20T18:31:06.275730Z","shell.execute_reply":"2024-12-20T19:56:18.486420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def concatenate_splits(corpora):\n    multi_corpus = DatasetDict()\n    for split in corpora[0].keys():\n        multi_corpus[split] = concatenate_datasets([corpus[split] for corpus in corpora]).shuffle(seed=42)\n    return multi_corpus","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:23:15.460681Z","iopub.execute_input":"2024-12-20T20:23:15.461001Z","iopub.status.idle":"2024-12-20T20:23:15.465732Z","shell.execute_reply.started":"2024-12-20T20:23:15.460975Z","shell.execute_reply":"2024-12-20T20:23:15.464724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corpora_encoded = concatenate_splits([panx_en_encoded, \npanx_fr_encoded, \npanx_de_encoded, \npanx_it_encoded])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:23:46.516740Z","iopub.execute_input":"2024-12-20T20:23:46.517602Z","iopub.status.idle":"2024-12-20T20:23:46.584519Z","shell.execute_reply.started":"2024-12-20T20:23:46.517566Z","shell.execute_reply":"2024-12-20T20:23:46.583579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def forward_pass_with_label(batch):\n    # Convert dict of lists to list of dicts suitable for data collator\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    # Pad inputs and labels and put all tensors on device\n    batch = data_collator(features)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    with torch.no_grad():\n        # Pass data through model\n        output = trained_model(input_ids, attention_mask)\n        # logit.size: [batch_size, sequence_length, classes]\n        # Predict class with largest logit value on classes axis\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n    return {\"predicted_label\": predicted_label}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:26:01.871971Z","iopub.execute_input":"2024-12-20T20:26:01.872990Z","iopub.status.idle":"2024-12-20T20:26:01.879224Z","shell.execute_reply.started":"2024-12-20T20:26:01.872945Z","shell.execute_reply":"2024-12-20T20:26:01.878207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_set_corpa = corpora_encoded[\"test\"]\ntest_set_corpa = test_set_corpa.map(forward_pass_with_label, batched=True, batch_size=32)\ndf = test_set_corpa.to_pandas()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:26:03.296640Z","iopub.execute_input":"2024-12-20T20:26:03.297011Z","iopub.status.idle":"2024-12-20T20:27:09.231476Z","shell.execute_reply.started":"2024-12-20T20:26:03.296979Z","shell.execute_reply":"2024-12-20T20:27:09.230678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index2tag[-100] = \"IGN\"\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(\n lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(\n lambda x: [index2tag[i] for i in x])\ndf[\"labels\"] = df[\"labels\"].apply(\n lambda x: [index2tag[i] for i in x])\ndf['predicted_label'] = df.apply(\n lambda x: x['predicted_label'][:len(x['input_ids'])], axis=1)\ndf.head(1)\ndf_tokens = df.apply(pd.Series.explode)\ndf_tokens = df_tokens.query(\"labels != 'IGN'\")\ndf_tokens.head(7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:27:09.233183Z","iopub.execute_input":"2024-12-20T20:27:09.233532Z","iopub.status.idle":"2024-12-20T20:27:10.883011Z","shell.execute_reply.started":"2024-12-20T20:27:09.233499Z","shell.execute_reply":"2024-12-20T20:27:10.881826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\n    \nplot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"],tags.names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:27:10.884175Z","iopub.execute_input":"2024-12-20T20:27:10.884533Z","iopub.status.idle":"2024-12-20T20:27:12.439779Z","shell.execute_reply.started":"2024-12-20T20:27:10.884496Z","shell.execute_reply":"2024-12-20T20:27:12.438894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T20:32:39.224260Z","iopub.execute_input":"2024-12-20T20:32:39.225036Z","iopub.status.idle":"2024-12-20T20:32:40.491367Z","shell.execute_reply.started":"2024-12-20T20:32:39.224999Z","shell.execute_reply":"2024-12-20T20:32:40.490266Z"}},"outputs":[],"execution_count":null}]}